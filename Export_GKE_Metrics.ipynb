{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# HPA Workload Recommender - Export Metrics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "This Colab can be used for export GKE metrics from Cloud Monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8925ff9e165e"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2b4ef9b72d43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726534539356,
          "user_tz": 240,
          "elapsed": 21501,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "f8bcbbb4-797b-4331-c11f-4ec6f6c76ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/200.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/200.9 kB\u001b[0m \u001b[31m675.9 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/200.9 kB\u001b[0m \u001b[31m757.9 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m163.8/200.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.9/200.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.34.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.2 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip3 install --upgrade --quiet click google-auth urllib3 requests pandas pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f200f10a1da3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726535036482,
          "user_tz": 240,
          "elapsed": 152,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b49231643e4"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7176ea64999b"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7de6ef0fac42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726535049341,
          "user_tz": 240,
          "elapsed": 169,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4fdcb344-cfd1-47ef-8ce3-f267439935f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oM1iC_MfAts1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726535713885,
          "user_tz": 240,
          "elapsed": 1359,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"gke-opt-demo\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "CLUSTER_NAME = \"online-shop\"  # @param {type:\"string\"}\n",
        "NAMESPACE = \"default\"  # @param {type:\"string\"}\n",
        "CONTROLLER_NAME = \"frontend\"  # @param {type:\"string\n",
        "\n",
        "# Query Period\n",
        "ANALYSIS_START_DATETIME = '2024-08-16T00:00:00Z'  # @param {type:\"string\"}\n",
        "ANALYSIS_END_DATETIME = '2024-09-16T00:00:00Z'  # @param {type:\"string\"}\n",
        "\n",
        "# Retrieve the project number\n",
        "PROJECT_NUMBER = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
        "PROJECT_NUMBER = PROJECT_NUMBER[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project {PROJECT_ID}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXPr_9H-jInw",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726535746417,
          "user_tz": 240,
          "elapsed": 1265,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "cb60be40-a4e5-45ae-f03a-9077e7e26100"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiPLcVkFi6Q2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726535891820,
          "user_tz": 240,
          "elapsed": 16757,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "b00c65dd-e22b-412d-cba4-796902e23737"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are running on a Google Compute Engine virtual machine.\n",
            "The service credentials associated with this virtual machine\n",
            "will automatically be used by Application Default\n",
            "Credentials, so it is not necessary to use this command.\n",
            "\n",
            "If you decide to proceed anyway, your user credentials may be visible\n",
            "to others with access to this virtual machine. Are you sure you want\n",
            "to authenticate with your personal account?\n",
            "\n",
            "Do you want to continue (Y/n)?  Y\n",
            "\n",
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=29qJ8E4BsKNNLUEpksrlVdsYA3wKtB&prompt=consent&token_usage=remote&access_type=offline&code_challenge=MDYEg_ZIypy22dLM5AFScDXVCHYdbAk0ZztX2o_z_cI&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AQlEd8yIqG6W1v17napDDCbbwLTwqLS3d3UISd-iww8iP5wxbK37dZFXCxE-A2W1olNOfw\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\n",
            "Quota project \"gke-opt-demo\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import json\n",
        "import logging\n",
        "from google.auth import default\n",
        "from google.auth.transport.requests import Request\n",
        "from googleapiclient.discovery import build\n",
        "import os\n",
        "from datetime import datetime\n",
        "import uuid"
      ],
      "metadata": {
        "id": "TXasATQXm271",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726536921863,
          "user_tz": 240,
          "elapsed": 168,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Pod Startup Time from Asset Inventory"
      ],
      "metadata": {
        "id": "AaqiFUpllkNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get GKE Pod startup time\n",
        "def fetch_and_process_assets(project_id, location, cluster_name, controller_name, namespace):\n",
        "    \"\"\"\n",
        "    Fetches Kubernetes asset inventory data from Google Cloud API and returns it as a DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the API client\n",
        "    try:\n",
        "        credentials, project_id = default()\n",
        "        credentials.refresh(Request())\n",
        "        service = build('cloudasset', 'v1', credentials=credentials)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to authenticate and initialize the Google Cloud Asset API: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    asset_data = []\n",
        "    next_page_token = None\n",
        "\n",
        "    # Paginate through all pages of results\n",
        "    while True:\n",
        "        # Make the request for asset inventory with pagination handling\n",
        "        try:\n",
        "            request = service.assets().list(\n",
        "                parent=f\"projects/{project_id}\",\n",
        "                assetTypes=[\"k8s.io/Pod\"],\n",
        "                contentType=\"RESOURCE\",\n",
        "                pageToken=next_page_token\n",
        "            )\n",
        "            response = request.execute()\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching asset inventory: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Process the current page's response and extract relevant data\n",
        "        if 'assets' in response:\n",
        "            for asset in response['assets']:\n",
        "                name = asset.get('name', '')\n",
        "\n",
        "                # Construct the expected name format\n",
        "                expected_name = f'projects/{project_id}/locations/{location}/clusters/{cluster_name}/k8s/namespaces/{namespace}/pods/{controller_name}'\n",
        "\n",
        "                # Check if the name exactly matches the constructed expected name\n",
        "                if expected_name in name:  # Use `startswith` to handle pod suffix like '-68f5d8498d-67bzg'\n",
        "                    resource = asset.get('resource', {}).get('data', {})\n",
        "\n",
        "                    # Extract container information and check for readinessProbe\n",
        "                    containers_info = extract_container_info(resource, location, cluster_name, namespace, controller_name, project_id)\n",
        "\n",
        "                    # Add the containers info to the asset data\n",
        "                    if containers_info:\n",
        "                        asset_data.extend(containers_info)\n",
        "\n",
        "        # Check if there is a next page token\n",
        "        next_page_token = response.get('nextPageToken', None)\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    # Convert the list of asset data into a DataFrame\n",
        "    return pd.DataFrame(asset_data)\n",
        "\n",
        "\n",
        "def extract_container_info(resource, location, cluster_name, namespace, controller_name, project_id):\n",
        "    \"\"\"\n",
        "    Extracts the workload information from a resource and checks if readinessProbe exists.\n",
        "    Also includes metadata like location, cluster_name, namespace, etc.\n",
        "\n",
        "    Parameters:\n",
        "    - resource (dict): The resource data from the asset API response.\n",
        "    - location (str): The location to filter by.\n",
        "    - cluster_name (str): The cluster name to filter by.\n",
        "    - namespace (str): The namespace to filter by.\n",
        "    - controller_name (str): The controller name to filter by.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of dictionaries containing container information, readinessProbe, status conditions, and metadata.\n",
        "    \"\"\"\n",
        "    container_list = []\n",
        "\n",
        "    # Ensure that all relevant fields are present before proceeding\n",
        "    if 'spec' not in resource or 'status' not in resource:\n",
        "        return []\n",
        "\n",
        "    # Extract container information\n",
        "    for container in resource.get('spec', {}).get('containers', []):\n",
        "        container_info = {}\n",
        "\n",
        "        # Add metadata (location, cluster_name, namespace, etc.)\n",
        "        container_info['project_id'] = project_id\n",
        "        container_info['location'] = location\n",
        "        container_info['cluster_name'] = cluster_name\n",
        "        container_info['namespace'] = namespace\n",
        "        container_info['controller_name'] = controller_name\n",
        "        container_info['container_name'] = container.get('name', 'Unknown')\n",
        "        container_info['readiness_probe_exists'] = 'readinessProbe' in container\n",
        "\n",
        "        # Extract status conditions with lastTransitionTime\n",
        "        status_conditions = extract_status_conditions(resource.get('status', {}).get('conditions', []))\n",
        "        container_info.update(status_conditions)\n",
        "\n",
        "        # Append to the list of containers\n",
        "        container_list.append(container_info)\n",
        "\n",
        "    return container_list\n",
        "\n",
        "\n",
        "def extract_status_conditions(conditions):\n",
        "    \"\"\"\n",
        "    Extracts the 'PodScheduled' and 'Ready' conditions and their lastTransitionTime from a list of conditions.\n",
        "\n",
        "    Parameters:\n",
        "    - conditions (list): A list of status conditions.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary with 'PodScheduled' and 'Ready' statuses and lastTransitionTimes.\n",
        "    \"\"\"\n",
        "    status_conditions = {\n",
        "        'PodScheduled_lastTransitionTime': 'Unknown',\n",
        "        'Ready_lastTransitionTime': 'Unknown'\n",
        "    }\n",
        "\n",
        "    for condition in conditions:\n",
        "        if condition['type'] == 'PodScheduled':\n",
        "            status_conditions['PodScheduled_lastTransitionTime'] = condition.get('lastTransitionTime', 'Unknown')\n",
        "        elif condition['type'] == 'Ready':\n",
        "            status_conditions['Ready_lastTransitionTime'] = condition.get('lastTransitionTime', 'Unknown')\n",
        "\n",
        "    return status_conditions\n"
      ],
      "metadata": {
        "id": "K6aaRGPSlUnM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726537567458,
          "user_tz": 240,
          "elapsed": 161,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4481de089cc2"
      },
      "source": [
        "## Export GKE Metrics from Cloud Monitoring\n",
        "\n",
        "Export the following [GKE metrics](https://https://cloud.google.com/monitoring/api/metrics_kubernetes#kubernetes) from Cloud Monitoring and save to a file\n",
        "- kubernetes.io/container/cpu/core_usage_time\n",
        "- kubernetes.io/container/memory/used_bytes\n",
        "- kubernetes.io/container/cpu/request_cores\n",
        "- kubernetes.io/container/memory/request_bytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save Files\n",
        "def save_dataframes(output_dir: Path, format: str, metrics_data: dict, prefix: str, zip_files: bool):\n",
        "    \"\"\"\n",
        "    Save all dataframes (metrics and asset) to disk in the specified format, and optionally zip the files.\n",
        "\n",
        "    Args:\n",
        "    - output_dir (Path): Directory where files will be saved.\n",
        "    - format (str): File format, either 'csv' or 'parquet'.\n",
        "    - metrics_data (dict): Dictionary of metrics dataframes.\n",
        "    - prefix (str): Unique prefix for all files.\n",
        "    - zip_files (bool): If True, zip the files after saving.\n",
        "    \"\"\"\n",
        "\n",
        "    # Save all metric dataframes\n",
        "    for metric_name, df in metrics_data.items():\n",
        "        file_path = output_dir / f\"{prefix}_{metric_name}.{format}\"\n",
        "        try:\n",
        "            if format == 'csv':\n",
        "                df.to_csv(file_path, index=False)\n",
        "            elif format == 'parquet':\n",
        "                df.to_parquet(file_path, index=False)\n",
        "            print(f\"Saved {metric_name} metrics to {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save {metric_name} metrics to {file_path}. Error: {e}\")\n",
        "\n",
        "\n",
        "    # Optionally zip the files\n",
        "    if zip_files:\n",
        "        zip_file_path = output_dir.parent / f\"{prefix}_metrics_and_assets.zip\"\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
        "                for root, _, files in os.walk(output_dir):\n",
        "                    for file in files:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        zipf.write(file_path, os.path.relpath(file_path, output_dir))\n",
        "            print(f\"Zipped files into {zip_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to zip files into {zip_file_path}. Error: {e}\")\n"
      ],
      "metadata": {
        "id": "NgPRnChqmtXA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726537571445,
          "user_tz": 240,
          "elapsed": 173,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6031842560d2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726537558987,
          "user_tz": 240,
          "elapsed": 167,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "#@title Get GKE Metrics\n",
        "\n",
        "# Exclude namespaces that should not be included in the metrics gathering\n",
        "EXCLUDED_NAMESPACES = [\n",
        "    \"kube-system\", \"istio-system\", \"gatekeeper-system\", \"gke-system\",\n",
        "    \"gmp-system\", \"gke-gmp-system\", \"gke-managed-filestorecsi\", \"gke-mcs\"\n",
        "]\n",
        "\n",
        "# Fields to be used in the groupBy in the API query\n",
        "GROUP_BY_FIELDS = [\n",
        "    \"resource.labels.project_id\",\n",
        "    \"resource.labels.location\",\n",
        "    \"resource.labels.cluster_name\",\n",
        "    \"resource.labels.namespace_name\",\n",
        "    \"resource.labels.container_name\",\n",
        "    \"resource.labels.pod_name\",\n",
        "    \"metadata.system_labels.top_level_controller_name\",\n",
        "    \"metadata.system_labels.top_level_controller_type\"\n",
        "\n",
        "]\n",
        "\n",
        "def build_filter_string(\n",
        "    metric: str,\n",
        "    project_id: str = '',\n",
        "    location: str = '',\n",
        "    cluster_name: str = '',\n",
        "    namespace: str = '',\n",
        "    container_name: str = '',\n",
        "    controller_name: str = '',\n",
        "    controller_type: str = ''\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Constructs a filter string for querying based on provided parameters.\n",
        "\n",
        "    Parameters:\n",
        "    - metric (str): The metric type to be used in the filter.\n",
        "    - project_id (str): The project ID for the filter.\n",
        "    - location (str): The location for the filter.\n",
        "    - cluster_name (str): The cluster name for the filter.\n",
        "    - namespace (str): The namespace for the filter.\n",
        "    - container_name (str): The container name for the filter.\n",
        "    - controller_name (str): The controller name for the filter.\n",
        "    - controller_type (str): The controller type for the filter.\n",
        "\n",
        "    Returns:\n",
        "    - str: A constructed filter string.\n",
        "    \"\"\"\n",
        "    filter_conditions = [\n",
        "        f'metric.type = \"{metric}\"',\n",
        "        'resource.type = \"k8s_container\"'\n",
        "    ]\n",
        "\n",
        "    if 'memory/used_bytes' in metric.lower():\n",
        "        filter_conditions.append('metric.label.memory_type = \"non-evictable\"')\n",
        "\n",
        "    if project_id:\n",
        "        filter_conditions.append(f'resource.labels.project_id = \"{project_id}\"')\n",
        "\n",
        "    if location:\n",
        "        filter_conditions.append(f'resource.labels.location = \"{location}\"')\n",
        "\n",
        "    if cluster_name:\n",
        "        filter_conditions.append(f'resource.labels.cluster_name = \"{cluster_name}\"')\n",
        "\n",
        "    if namespace:\n",
        "        filter_conditions.append(f'resource.labels.namespace_name = \"{namespace}\"')\n",
        "\n",
        "    if container_name:\n",
        "        filter_conditions.append(f'resource.labels.container_name = \"{container_name}\"')\n",
        "    if controller_name:\n",
        "        filter_conditions.append(f'metadata.system_labels.top_level_controller_name = \"{controller_name}\"')\n",
        "    if controller_type:\n",
        "        filter_conditions.append(f'metadata.system_labels.top_level_controller_type = \"{controller_type}\"')\n",
        "\n",
        "    # Exclude unwanted namespaces\n",
        "    excluded_filter = ' AND '.join(\n",
        "        f'NOT resource.labels.namespace_name = \"{namespace}\"' for namespace in EXCLUDED_NAMESPACES\n",
        "    )\n",
        "    filter_conditions.append(excluded_filter)\n",
        "\n",
        "    return ' AND '.join(filter_conditions)\n",
        "\n",
        "def fetch_metrics_from_api(\n",
        "    project_id, location, cluster_name, namespace, container_name,\n",
        "    controller_name, controller_type, metric, start_time, end_time,\n",
        "    per_series_aligner, cross_series_reducer):\n",
        "    \"\"\"\n",
        "    Fetches metrics from Google Cloud Monitoring API based on the provided parameters.\n",
        "    \"\"\"\n",
        "    # Initialize the API client\n",
        "    try:\n",
        "        credentials, project_id = default()\n",
        "        credentials.refresh(Request())\n",
        "        service = build('monitoring', 'v3', credentials=credentials)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to authenticate and initialize the Google Cloud Asset API: {e}\")\n",
        "        return\n",
        "\n",
        "    filter_ = build_filter_string(\n",
        "        metric=metric,\n",
        "        project_id=project_id,\n",
        "        location=location,\n",
        "        cluster_name=cluster_name,\n",
        "        namespace=namespace,\n",
        "        container_name=container_name,\n",
        "        controller_name=controller_name,\n",
        "        controller_type=controller_type\n",
        "    )\n",
        "\n",
        "    print(f\"Fetching data for metric: {metric} ...\")\n",
        "\n",
        "    try:\n",
        "        all_time_series_data = []\n",
        "        request = service.projects().timeSeries().list(\n",
        "            name=f\"projects/{project_id}\",\n",
        "            aggregation_alignmentPeriod=\"60s\",\n",
        "            aggregation_crossSeriesReducer=cross_series_reducer,\n",
        "            aggregation_groupByFields=GROUP_BY_FIELDS,\n",
        "            aggregation_perSeriesAligner=per_series_aligner,\n",
        "            filter=filter_,\n",
        "            interval_endTime=end_time,\n",
        "            interval_startTime=start_time\n",
        "        )\n",
        "\n",
        "        while request is not None:\n",
        "            response = request.execute()\n",
        "            all_time_series_data.extend(response.get('timeSeries', []))\n",
        "            nextPageToken = response.get('nextPageToken')\n",
        "\n",
        "            request = service.projects().timeSeries().list_next(previous_request=request, previous_response=response) if nextPageToken else None\n",
        "\n",
        "        df = pd.json_normalize(\n",
        "            all_time_series_data,\n",
        "            record_path='points',\n",
        "            meta=[\n",
        "                ['metric', 'type'],\n",
        "                ['resource', 'type'],\n",
        "                ['resource', 'labels', 'project_id'],\n",
        "                ['resource', 'labels', 'location'],\n",
        "                ['resource', 'labels', 'cluster_name'],\n",
        "                ['resource', 'labels', 'namespace_name'],\n",
        "                ['resource', 'labels', 'container_name'],\n",
        "                ['resource', 'labels', 'pod_name'],\n",
        "                ['metadata', 'systemLabels', 'top_level_controller_name'],\n",
        "                ['metadata', 'systemLabels', 'top_level_controller_type']\n",
        "            ],\n",
        "            errors='ignore'\n",
        "        )\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data found for metric: {metric}\")\n",
        "        else:\n",
        "            print(f\"Successfully fetched.\")\n",
        "\n",
        "        return df if not df.empty else pd.DataFrame()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching metrics: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def fetch_all_metrics(\n",
        "    project_id, location, cluster_name, namespace, container_name,\n",
        "    controller_name, controller_type, start_time, end_time, metrics_info):\n",
        "    \"\"\"\n",
        "    Fetch all required metrics as per the metrics info configuration.\n",
        "    \"\"\"\n",
        "    print(f\"Starting to fetch metrics for the following configuration: \"\n",
        "               f\"Project ID: {project_id}, Location: {location}, \"\n",
        "               f\"Cluster Name: {cluster_name}, Namespace: {namespace}, \"\n",
        "               f\"Controller Name: {controller_name}\")\n",
        "\n",
        "    all_metrics_data = {}\n",
        "\n",
        "    for key, info in metrics_info.items():\n",
        "\n",
        "        metric_type = info[\"metric_type\"]\n",
        "        aligner = info.get(\"aligner\", \"ALIGN_MEAN\")\n",
        "        reducer = info.get(\"reducer\", \"REDUCE_MEAN\")\n",
        "\n",
        "        metric_data = fetch_metrics_from_api(\n",
        "            project_id, location, cluster_name, namespace, container_name,\n",
        "            controller_name, controller_type, metric_type, start_time, end_time,\n",
        "            aligner, reducer\n",
        "        )\n",
        "\n",
        "        if not metric_data.empty:\n",
        "            all_metrics_data[key] = metric_data\n",
        "        else:\n",
        "            print(f\"No data found for {metric_type}.\")\n",
        "\n",
        "        # Fetch pod startup time from Asset Inventory\n",
        "        all_metrics_data['pod_startup']  = fetch_and_process_assets(\n",
        "            project_id,\n",
        "            location,\n",
        "            cluster_name,\n",
        "            controller_name,\n",
        "            namespace\n",
        "            )\n",
        "\n",
        "    print(\"Completed fetching all metrics.\")\n",
        "    return all_metrics_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "f35e48bf1b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726537697673,
          "user_tz": 240,
          "elapsed": 92826,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "04c6e5c0-9290-4bc8-a7d2-eda426940054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to fetch metrics for the following configuration: Project ID: gke-opt-demo, Location: us-central1, Cluster Name: online-shop, Namespace: default, Controller Name: frontend\n",
            "Fetching data for metric: kubernetes.io/container/cpu/core_usage_time ...\n",
            "Successfully fetched.\n",
            "Fetching data for metric: kubernetes.io/container/memory/used_bytes ...\n",
            "Successfully fetched.\n",
            "Fetching data for metric: kubernetes.io/container/cpu/request_cores ...\n",
            "Successfully fetched.\n",
            "Fetching data for metric: kubernetes.io/container/memory/request_bytes ...\n",
            "Successfully fetched.\n",
            "Completed fetching all metrics.\n",
            "Saved cpu_usage metrics to output/20240917_1687/20240917_1687_cpu_usage.parquet\n",
            "Saved pod_startup metrics to output/20240917_1687/20240917_1687_pod_startup.parquet\n",
            "Saved memory_usage metrics to output/20240917_1687/20240917_1687_memory_usage.parquet\n",
            "Saved cpu_request metrics to output/20240917_1687/20240917_1687_cpu_request.parquet\n",
            "Saved memory_request metrics to output/20240917_1687/20240917_1687_memory_request.parquet\n",
            "Zipped files into output/20240917_1687_metrics_and_assets.zip\n"
          ]
        }
      ],
      "source": [
        "#@title Save GKE Metrics to File\n",
        "def main(project_id, location, cluster_name, namespace, controller_name,\n",
        "         start_time, end_time, format, zip_files, output_dir):\n",
        "    \"\"\"Fetch GKE metrics, save each metric type to its own file, optionally fetch the asset inventory, and optionally zip all files into one folder.\"\"\"\n",
        "\n",
        "    unique_prefix = f\"{datetime.now().strftime('%Y%m%d')}_{uuid.uuid4().hex[:4]}\"\n",
        "\n",
        "    # Load configuration and set up storage directory\n",
        "\n",
        "    storage_dir = Path(output_dir) if output_dir else '.'\n",
        "\n",
        "    # Ensure storage directory exists\n",
        "    if not storage_dir.exists():\n",
        "        storage_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Full path for output files\n",
        "    output_dir = storage_dir / unique_prefix\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Metric Info Configuration for Fetching Multiple Metrics\n",
        "    metrics_info = {\n",
        "        \"cpu_usage\": {\n",
        "            \"metric_type\": \"kubernetes.io/container/cpu/core_usage_time\",\n",
        "            \"aligner\": \"ALIGN_RATE\",\n",
        "            \"reducer\": \"REDUCE_MEAN\",\n",
        "        },\n",
        "        \"memory_usage\": {\n",
        "            \"metric_type\": \"kubernetes.io/container/memory/used_bytes\",\n",
        "            \"aligner\": \"ALIGN_MAX\",\n",
        "            \"reducer\": \"REDUCE_MAX\",\n",
        "        },\n",
        "        \"cpu_request\": {\n",
        "            \"metric_type\": \"kubernetes.io/container/cpu/request_cores\",\n",
        "            \"aligner\": \"ALIGN_MEAN\",\n",
        "            \"reducer\": \"REDUCE_MEAN\",\n",
        "        },\n",
        "        \"memory_request\": {\n",
        "            \"metric_type\": \"kubernetes.io/container/memory/request_bytes\",\n",
        "            \"aligner\": \"ALIGN_MEAN\",\n",
        "            \"reducer\": \"REDUCE_MEAN\",\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Initialize placeholders for data\n",
        "    all_metrics_data = {}\n",
        "\n",
        "\n",
        "    # Try block for fetching both metrics and asset inventory data\n",
        "    try:\n",
        "        # Fetch GKE metrics\n",
        "        all_metrics_data = fetch_all_metrics(\n",
        "            project_id=project_id,\n",
        "            location=location,\n",
        "            cluster_name=cluster_name,\n",
        "            namespace=namespace,\n",
        "            container_name='',\n",
        "            controller_name=controller_name,\n",
        "            controller_type='Deployment',\n",
        "            start_time=start_time,\n",
        "            end_time=end_time,\n",
        "            metrics_info=metrics_info\n",
        "        )\n",
        "        if not all_metrics_data:\n",
        "            print(\"No metrics data found. Please ensure the parameters are correct.\")\n",
        "            return\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data: {e}. Please check your input parameters.\")\n",
        "        return\n",
        "\n",
        "    # Save all dataframes (metrics and assets), and optionally zip them\n",
        "    save_dataframes(output_dir, format, all_metrics_data, unique_prefix, zip_files)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # file options csv and parquet\n",
        "    main(PROJECT_ID, LOCATION, CLUSTER_NAME, NAMESPACE, CONTROLLER_NAME, ANALYSIS_START_DATETIME, ANALYSIS_END_DATETIME, 'parquet', True, 'output')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the .zip file"
      ],
      "metadata": {
        "id": "QYmQmgY2q5Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download .zip file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpCuDfsEicpG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726537742657,
          "user_tz": 240,
          "elapsed": 163,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "039d8576-f37f-4e5d-ff27-cc75d026fde2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ray_cluster_management.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}